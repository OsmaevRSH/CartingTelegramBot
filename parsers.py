import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List
from models import Race, DayRaces, Cart, ParsingError


class ArchiveParser:
    """–ü–∞—Ä—Å–µ—Ä –∞—Ä—Ö–∏–≤–∞ –∑–∞–µ–∑–¥–æ–≤"""
    
    def __init__(self):
        self.url_string = "https://mayak.kartchrono.com/archive/"
    
    def parse(self) -> List[DayRaces]:
        """–ü–∞—Ä—Å–∏—Ç –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∞—Ä—Ö–∏–≤–∞"""
        try:
            response = requests.get(self.url_string)
            response.raise_for_status()
            
            return self._parse_html(response.text)
        except requests.RequestException as e:
            raise ParsingError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        except Exception as e:
            raise ParsingError(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
    
    def _parse_html(self, html: str) -> List[DayRaces]:
        """–ü–∞—Ä—Å–∏—Ç HTML –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –∑–∞–µ–∑–¥–∞—Ö"""
        soup = BeautifulSoup(html, 'html.parser')
        day_races = []
        
        # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–ª–∞—Å—Å–æ–º archiveData
        archive_data = soup.find(class_="archiveData")
        if not archive_data:
            raise ParsingError("–ù–µ –Ω–∞–π–¥–µ–Ω —ç–ª–µ–º–µ–Ω—Ç archiveData")
        
        race_date = None
        races = []
        
        for element in archive_data.children:
            if hasattr(element, 'get') and element.get('class'):
                # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –¥–∞—Ç–æ–π
                if 'archiveDateHeader' in element.get('class', []):
                    if race_date:
                        day_races.append(DayRaces(date=race_date, races=races))
                        races = []
                    
                    date_text = element.get_text().strip()
                    try:
                        race_date = datetime.strptime(date_text, "%d.%m.%Y")
                    except ValueError:
                        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É: {date_text}")
                        continue
                
                # –°—Ç—Ä–æ–∫–∞ —Å –¥–∞–Ω–Ω—ã–º–∏ –∑–∞–µ–∑–¥–∞
                elif 'archiveDataRow' in element.get('class', []):
                    link_element = element.find('a')
                    if link_element:
                        href = link_element.get('href', '')
                        # –ò—â–µ–º –Ω–æ–º–µ—Ä –∑–∞–µ–∑–¥–∞ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Å—Å—ã–ª–∫–∏
                        race_number_element = link_element.find()
                        if race_number_element:
                            race_number_element = race_number_element.find()
                            if race_number_element:
                                race_number = race_number_element.get_text().strip()
                            else:
                                race_number = link_element.get_text().strip()
                        else:
                            race_number = link_element.get_text().strip()
                        
                        races.append(Race(number=race_number, href=href))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É, –µ—Å–ª–∏ –µ—Å—Ç—å
        if race_date and races:
            day_races.append(DayRaces(date=race_date, races=races))
        
        return day_races


class RaceParser:
    """–ü–∞—Ä—Å–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–∞–µ–∑–¥–∞"""
    
    def __init__(self):
        self.url_string = "https://mayak.kartchrono.com/archive/"
    
    def parse(self, href: str) -> List[Cart]:
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–∞–µ–∑–¥–∞"""
        try:
            url = self.url_string + href
            print(f"üîó –ü–∞—Ä—Å–∏–º URL: {url}")
            
            response = requests.get(url)
            response.raise_for_status()
            
            return self._parse_html(response.text)
        except requests.RequestException as e:
            raise ParsingError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        except Exception as e:
            raise ParsingError(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
    
    def _parse_html(self, html: str) -> List[Cart]:
        """–ü–∞—Ä—Å–∏—Ç HTML –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–µ–∑–¥–∞"""
        soup = BeautifulSoup(html, 'html.parser')
        race_carts = []
        
        # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        results_table = soup.find(id="resultsTable")
        if not results_table:
            raise ParsingError("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏")
        
        # –ò—â–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
        data_rows = results_table.find_all(class_="dataRow")
        
        for row in data_rows:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏
            number_element = row.find(id="num")
            best_lap_element = row.find(id="best_lap_time")
            position_element = row.find(id="pos")
            
            number = number_element.get_text().strip() if number_element else ""
            best_lap = best_lap_element.get_text().strip() if best_lap_element else ""
            position = position_element.get_text().strip() if position_element else ""
            
            race_carts.append(Cart(
                id="",  # competitorid –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Swift –∫–æ–¥–µ
                number=number,
                best_lap=best_lap,
                position=position
            ))
        
        return race_carts 